{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8a8925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train-Validation split complete\n",
      "Train shape: (6932, 31)  Validation shape: (771, 31)\n",
      "\n",
      "Class distribution in train set:\n",
      " tfopwg_disp\n",
      "5    0.607473\n",
      "3    0.155366\n",
      "1    0.088719\n",
      "4    0.075736\n",
      "0    0.060012\n",
      "2    0.012695\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in val set:\n",
      " tfopwg_disp\n",
      "5    0.607004\n",
      "3    0.155642\n",
      "1    0.089494\n",
      "4    0.075227\n",
      "0    0.059663\n",
      "2    0.012970\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed ML-ready CSV\n",
    "df = pd.read_csv(\"tess_toi_ml_ready.csv\")\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df.drop(columns=[\"tfopwg_disp\"])\n",
    "y = df[\"tfopwg_disp\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.1,\n",
    "    stratify=y,   # ensures proportional class distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save splits (optional)\n",
    "# X_train.to_csv(\"X_train.csv\", index=False)\n",
    "# X_val.to_csv(\"X_val.csv\", index=False)\n",
    "# y_train.to_csv(\"y_train.csv\", index=False)\n",
    "# y_val.to_csv(\"y_val.csv\", index=False)\n",
    "\n",
    "print(\"✅ Train-Validation split complete\")\n",
    "print(\"Train shape:\", X_train.shape, \" Validation shape:\", X_val.shape)\n",
    "print(\"\\nClass distribution in train set:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in val set:\\n\", y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b069db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cuML not found - Falling back to CPU Logistic Regression\n",
      "[INFO] Dataset loaded. Shape: (7703, 32)\n",
      "[INFO] Target classes: [0, 1, 2, 3, 4, 5]\n",
      "[INFO] Train shape: (6162, 31), Validation shape: (1541, 31)\n",
      "[INFO] Features standardized\n",
      "[INFO] Training Logistic Regression...\n",
      "[INFO] Training completed!\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[  6   3   0  10   5  68]\n",
      " [  4  50   1   1   1  80]\n",
      " [  0   0   0   6   0  14]\n",
      " [  5   3   0  64   3 164]\n",
      " [  1   4   0   3  19  90]\n",
      " [  6  27   1  27   6 869]]\n",
      "\n",
      "Class: 0\n",
      "TP: 6, TN: 1433, FP: 16, FN: 86\n",
      "Accuracy: 0.9338\n",
      "Error Rate: 0.0662\n",
      "Precision: 0.2727\n",
      "Recall / TPR: 0.0652\n",
      "F1 Score: 0.1053\n",
      "Specificity / TNR: 0.9890\n",
      "False Positive Rate (FPR): 0.0110\n",
      "False Negative Rate (FNR): 0.9348\n",
      "\n",
      "Class: 1\n",
      "TP: 50, TN: 1367, FP: 37, FN: 87\n",
      "Accuracy: 0.9195\n",
      "Error Rate: 0.0805\n",
      "Precision: 0.5747\n",
      "Recall / TPR: 0.3650\n",
      "F1 Score: 0.4464\n",
      "Specificity / TNR: 0.9736\n",
      "False Positive Rate (FPR): 0.0264\n",
      "False Negative Rate (FNR): 0.6350\n",
      "\n",
      "Class: 2\n",
      "TP: 0, TN: 1519, FP: 2, FN: 20\n",
      "Accuracy: 0.9857\n",
      "Error Rate: 0.0143\n",
      "Precision: 0.0000\n",
      "Recall / TPR: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity / TNR: 0.9987\n",
      "False Positive Rate (FPR): 0.0013\n",
      "False Negative Rate (FNR): 1.0000\n",
      "\n",
      "Class: 3\n",
      "TP: 64, TN: 1255, FP: 47, FN: 175\n",
      "Accuracy: 0.8559\n",
      "Error Rate: 0.1441\n",
      "Precision: 0.5766\n",
      "Recall / TPR: 0.2678\n",
      "F1 Score: 0.3657\n",
      "Specificity / TNR: 0.9639\n",
      "False Positive Rate (FPR): 0.0361\n",
      "False Negative Rate (FNR): 0.7322\n",
      "\n",
      "Class: 4\n",
      "TP: 19, TN: 1409, FP: 15, FN: 98\n",
      "Accuracy: 0.9267\n",
      "Error Rate: 0.0733\n",
      "Precision: 0.5588\n",
      "Recall / TPR: 0.1624\n",
      "F1 Score: 0.2517\n",
      "Specificity / TNR: 0.9895\n",
      "False Positive Rate (FPR): 0.0105\n",
      "False Negative Rate (FNR): 0.8376\n",
      "\n",
      "Class: 5\n",
      "TP: 869, TN: 189, FP: 416, FN: 67\n",
      "Accuracy: 0.6866\n",
      "Error Rate: 0.3134\n",
      "Precision: 0.6763\n",
      "Recall / TPR: 0.9284\n",
      "F1 Score: 0.7825\n",
      "Specificity / TNR: 0.3124\n",
      "False Positive Rate (FPR): 0.6876\n",
      "False Negative Rate (FNR): 0.0716\n",
      "\n",
      "[INFO] Logistic Regression evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Logistic Regression GPU/CPU Training Pipeline\n",
    "# ========================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try GPU (cuML), fallback to CPU (scikit-learn)\n",
    "try:\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.preprocessing.model_selection import train_test_split as cu_train_test_split\n",
    "    print(\"[INFO] cuML found - Using GPU Logistic Regression\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    print(\"[INFO] cuML not found - Falling back to CPU Logistic Regression\")\n",
    "    USE_GPU = False\n",
    "\n",
    "# ------------------------\n",
    "# Load dataset\n",
    "# ------------------------\n",
    "print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "X = df.drop(\"tfopwg_disp\", axis=1)\n",
    "y = df[\"tfopwg_disp\"]\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"[INFO] Target classes: {list(le.classes_)}\")\n",
    "\n",
    "# ------------------------\n",
    "# Train-test split (stratified)\n",
    "# ------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"[INFO] Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Scale features\n",
    "# ------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "print(\"[INFO] Features standardized\")\n",
    "\n",
    "# ------------------------\n",
    "# Train Logistic Regression\n",
    "# ------------------------\n",
    "if USE_GPU:\n",
    "    model = cuLogisticRegression(max_iter=1000, verbose=0)\n",
    "else:\n",
    "    model = LogisticRegression(max_iter=1000, verbose=0, n_jobs=-1)\n",
    "\n",
    "print(\"[INFO] Training Logistic Regression...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"[INFO] Training completed!\")\n",
    "\n",
    "# ------------------------\n",
    "# Predictions\n",
    "# ------------------------\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# ------------------------\n",
    "# Confusion Matrix & Metrics\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"\\n===== CONFUSION MATRIX =====\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate TP, TN, FP, FN for multi-class\n",
    "# Here, we'll calculate **per-class** metrics\n",
    "classes = le.classes_\n",
    "for idx, cls in enumerate(classes):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {cls}\")\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "    \n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Error_Rate = 1 - Accuracy\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
    "    Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "    FNR = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "    print(f\"Error Rate: {Error_Rate:.4f}\")\n",
    "    print(f\"Precision: {Precision:.4f}\")\n",
    "    print(f\"Recall / TPR: {Recall:.4f}\")\n",
    "    print(f\"F1 Score: {F1:.4f}\")\n",
    "    print(f\"Specificity / TNR: {Specificity:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {FNR:.4f}\")\n",
    "\n",
    "print(\"\\n[INFO] Logistic Regression evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78384ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cuML not found - Falling back to CPU Random Forest\n",
      "[INFO] Dataset loaded. Shape: (7703, 32)\n",
      "[INFO] Target classes: [0, 1, 2, 3, 4, 5]\n",
      "[INFO] Train shape: (6162, 31), Validation shape: (1541, 31)\n",
      "[INFO] Features standardized\n",
      "[INFO] Training Random Forest...\n",
      "[INFO] Training completed!\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[  4   1   0  17   1  69]\n",
      " [  0  51   0   3   7  76]\n",
      " [  0   0   0   5   0  15]\n",
      " [  0   1   0  88   2 148]\n",
      " [  0   3   0   2  38  74]\n",
      " [  2  14   1  26  10 883]]\n",
      "\n",
      "Class: 0\n",
      "TP: 4, TN: 1447, FP: 2, FN: 88\n",
      "Accuracy: 0.9416\n",
      "Error Rate: 0.0584\n",
      "Precision: 0.6667\n",
      "Recall / TPR: 0.0435\n",
      "F1 Score: 0.0816\n",
      "Specificity / TNR: 0.9986\n",
      "False Positive Rate (FPR): 0.0014\n",
      "False Negative Rate (FNR): 0.9565\n",
      "\n",
      "Class: 1\n",
      "TP: 51, TN: 1385, FP: 19, FN: 86\n",
      "Accuracy: 0.9319\n",
      "Error Rate: 0.0681\n",
      "Precision: 0.7286\n",
      "Recall / TPR: 0.3723\n",
      "F1 Score: 0.4928\n",
      "Specificity / TNR: 0.9865\n",
      "False Positive Rate (FPR): 0.0135\n",
      "False Negative Rate (FNR): 0.6277\n",
      "\n",
      "Class: 2\n",
      "TP: 0, TN: 1520, FP: 1, FN: 20\n",
      "Accuracy: 0.9864\n",
      "Error Rate: 0.0136\n",
      "Precision: 0.0000\n",
      "Recall / TPR: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Specificity / TNR: 0.9993\n",
      "False Positive Rate (FPR): 0.0007\n",
      "False Negative Rate (FNR): 1.0000\n",
      "\n",
      "Class: 3\n",
      "TP: 88, TN: 1249, FP: 53, FN: 151\n",
      "Accuracy: 0.8676\n",
      "Error Rate: 0.1324\n",
      "Precision: 0.6241\n",
      "Recall / TPR: 0.3682\n",
      "F1 Score: 0.4632\n",
      "Specificity / TNR: 0.9593\n",
      "False Positive Rate (FPR): 0.0407\n",
      "False Negative Rate (FNR): 0.6318\n",
      "\n",
      "Class: 4\n",
      "TP: 38, TN: 1404, FP: 20, FN: 79\n",
      "Accuracy: 0.9358\n",
      "Error Rate: 0.0642\n",
      "Precision: 0.6552\n",
      "Recall / TPR: 0.3248\n",
      "F1 Score: 0.4343\n",
      "Specificity / TNR: 0.9860\n",
      "False Positive Rate (FPR): 0.0140\n",
      "False Negative Rate (FNR): 0.6752\n",
      "\n",
      "Class: 5\n",
      "TP: 883, TN: 223, FP: 382, FN: 53\n",
      "Accuracy: 0.7177\n",
      "Error Rate: 0.2823\n",
      "Precision: 0.6980\n",
      "Recall / TPR: 0.9434\n",
      "F1 Score: 0.8024\n",
      "Specificity / TNR: 0.3686\n",
      "False Positive Rate (FPR): 0.6314\n",
      "False Negative Rate (FNR): 0.0566\n",
      "\n",
      "[INFO] Random Forest evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Random Forest GPU/CPU Training Pipeline\n",
    "# ========================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try GPU (cuML), fallback to CPU (scikit-learn)\n",
    "try:\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "    print(\"[INFO] cuML found - Using GPU Random Forest\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    print(\"[INFO] cuML not found - Falling back to CPU Random Forest\")\n",
    "    USE_GPU = False\n",
    "\n",
    "# ------------------------\n",
    "# Load dataset\n",
    "# ------------------------\n",
    "print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "X = df.drop(\"tfopwg_disp\", axis=1)\n",
    "y = df[\"tfopwg_disp\"]\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"[INFO] Target classes: {list(le.classes_)}\")\n",
    "\n",
    "# ------------------------\n",
    "# Train-test split (stratified)\n",
    "# ------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"[INFO] Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Scaling optional for RF, but keep for consistency\n",
    "# ------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "print(\"[INFO] Features standardized\")\n",
    "\n",
    "# ------------------------\n",
    "# Train Random Forest\n",
    "# ------------------------\n",
    "if USE_GPU:\n",
    "    model = cuRF(n_estimators=100, max_depth=12, verbose=0)\n",
    "else:\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=12, n_jobs=-1, verbose=0)\n",
    "\n",
    "print(\"[INFO] Training Random Forest...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"[INFO] Training completed!\")\n",
    "\n",
    "# ------------------------\n",
    "# Predictions\n",
    "# ------------------------\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# ------------------------\n",
    "# Confusion Matrix & Metrics\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"\\n===== CONFUSION MATRIX =====\")\n",
    "print(cm)\n",
    "\n",
    "# Multi-class metrics per class\n",
    "classes = le.classes_\n",
    "for idx, cls in enumerate(classes):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {cls}\")\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "    \n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Error_Rate = 1 - Accuracy\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
    "    Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "    FNR = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "    print(f\"Error Rate: {Error_Rate:.4f}\")\n",
    "    print(f\"Precision: {Precision:.4f}\")\n",
    "    print(f\"Recall / TPR: {Recall:.4f}\")\n",
    "    print(f\"F1 Score: {F1:.4f}\")\n",
    "    print(f\"Specificity / TNR: {Specificity:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {FNR:.4f}\")\n",
    "\n",
    "print(\"\\n[INFO] Random Forest evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dfdc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGBoost imported. Will attempt GPU usage if available.\n",
      "[INFO] Dataset loaded. Shape: (7703, 32)\n",
      "[INFO] Target classes: [0, 1, 2, 3, 4, 5]\n",
      "[INFO] Train shape: (6162, 31), Validation shape: (1541, 31)\n",
      "[INFO] Features standardized\n",
      "[INFO] Training XGBoost with GPU...\n",
      "[0]\tvalidation_0-mlogloss:1.66368\n",
      "[1]\tvalidation_0-mlogloss:1.56392\n",
      "[2]\tvalidation_0-mlogloss:1.48213\n",
      "[3]\tvalidation_0-mlogloss:1.41318\n",
      "[4]\tvalidation_0-mlogloss:1.35468\n",
      "[5]\tvalidation_0-mlogloss:1.30403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:00:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\tvalidation_0-mlogloss:1.25963\n",
      "[7]\tvalidation_0-mlogloss:1.22068\n",
      "[8]\tvalidation_0-mlogloss:1.18652\n",
      "[9]\tvalidation_0-mlogloss:1.15599\n",
      "[10]\tvalidation_0-mlogloss:1.12854\n",
      "[11]\tvalidation_0-mlogloss:1.10335\n",
      "[12]\tvalidation_0-mlogloss:1.08064\n",
      "[13]\tvalidation_0-mlogloss:1.06160\n",
      "[14]\tvalidation_0-mlogloss:1.04422\n",
      "[15]\tvalidation_0-mlogloss:1.02825\n",
      "[16]\tvalidation_0-mlogloss:1.01376\n",
      "[17]\tvalidation_0-mlogloss:0.99975\n",
      "[18]\tvalidation_0-mlogloss:0.98689\n",
      "[19]\tvalidation_0-mlogloss:0.97623\n",
      "[20]\tvalidation_0-mlogloss:0.96678\n",
      "[21]\tvalidation_0-mlogloss:0.95746\n",
      "[22]\tvalidation_0-mlogloss:0.94934\n",
      "[23]\tvalidation_0-mlogloss:0.94197\n",
      "[24]\tvalidation_0-mlogloss:0.93521\n",
      "[25]\tvalidation_0-mlogloss:0.92836\n",
      "[26]\tvalidation_0-mlogloss:0.92338\n",
      "[27]\tvalidation_0-mlogloss:0.91832\n",
      "[28]\tvalidation_0-mlogloss:0.91266\n",
      "[29]\tvalidation_0-mlogloss:0.90839\n",
      "[30]\tvalidation_0-mlogloss:0.90428\n",
      "[31]\tvalidation_0-mlogloss:0.90047\n",
      "[32]\tvalidation_0-mlogloss:0.89752\n",
      "[33]\tvalidation_0-mlogloss:0.89470\n",
      "[34]\tvalidation_0-mlogloss:0.89202\n",
      "[35]\tvalidation_0-mlogloss:0.88906\n",
      "[36]\tvalidation_0-mlogloss:0.88708\n",
      "[37]\tvalidation_0-mlogloss:0.88422\n",
      "[38]\tvalidation_0-mlogloss:0.88263\n",
      "[39]\tvalidation_0-mlogloss:0.88052\n",
      "[40]\tvalidation_0-mlogloss:0.87848\n",
      "[41]\tvalidation_0-mlogloss:0.87722\n",
      "[42]\tvalidation_0-mlogloss:0.87631\n",
      "[43]\tvalidation_0-mlogloss:0.87530\n",
      "[44]\tvalidation_0-mlogloss:0.87341\n",
      "[45]\tvalidation_0-mlogloss:0.87222\n",
      "[46]\tvalidation_0-mlogloss:0.87101\n",
      "[47]\tvalidation_0-mlogloss:0.87058\n",
      "[48]\tvalidation_0-mlogloss:0.86955\n",
      "[49]\tvalidation_0-mlogloss:0.86924\n",
      "[50]\tvalidation_0-mlogloss:0.86854\n",
      "[51]\tvalidation_0-mlogloss:0.86805\n",
      "[52]\tvalidation_0-mlogloss:0.86775\n",
      "[53]\tvalidation_0-mlogloss:0.86668\n",
      "[54]\tvalidation_0-mlogloss:0.86615\n",
      "[55]\tvalidation_0-mlogloss:0.86521\n",
      "[56]\tvalidation_0-mlogloss:0.86481\n",
      "[57]\tvalidation_0-mlogloss:0.86394\n",
      "[58]\tvalidation_0-mlogloss:0.86350\n",
      "[59]\tvalidation_0-mlogloss:0.86373\n",
      "[60]\tvalidation_0-mlogloss:0.86321\n",
      "[61]\tvalidation_0-mlogloss:0.86213\n",
      "[62]\tvalidation_0-mlogloss:0.86146\n",
      "[63]\tvalidation_0-mlogloss:0.86107\n",
      "[64]\tvalidation_0-mlogloss:0.86103\n",
      "[65]\tvalidation_0-mlogloss:0.86042\n",
      "[66]\tvalidation_0-mlogloss:0.86034\n",
      "[67]\tvalidation_0-mlogloss:0.86019\n",
      "[68]\tvalidation_0-mlogloss:0.86011\n",
      "[69]\tvalidation_0-mlogloss:0.86015\n",
      "[70]\tvalidation_0-mlogloss:0.85982\n",
      "[71]\tvalidation_0-mlogloss:0.85952\n",
      "[72]\tvalidation_0-mlogloss:0.85949\n",
      "[73]\tvalidation_0-mlogloss:0.85947\n",
      "[74]\tvalidation_0-mlogloss:0.85953\n",
      "[75]\tvalidation_0-mlogloss:0.85939\n",
      "[76]\tvalidation_0-mlogloss:0.85879\n",
      "[77]\tvalidation_0-mlogloss:0.85831\n",
      "[78]\tvalidation_0-mlogloss:0.85796\n",
      "[79]\tvalidation_0-mlogloss:0.85766\n",
      "[80]\tvalidation_0-mlogloss:0.85805\n",
      "[81]\tvalidation_0-mlogloss:0.85795\n",
      "[82]\tvalidation_0-mlogloss:0.85753\n",
      "[83]\tvalidation_0-mlogloss:0.85772\n",
      "[84]\tvalidation_0-mlogloss:0.85746\n",
      "[85]\tvalidation_0-mlogloss:0.85759\n",
      "[86]\tvalidation_0-mlogloss:0.85762\n",
      "[87]\tvalidation_0-mlogloss:0.85805\n",
      "[88]\tvalidation_0-mlogloss:0.85781\n",
      "[89]\tvalidation_0-mlogloss:0.85778\n",
      "[90]\tvalidation_0-mlogloss:0.85829\n",
      "[91]\tvalidation_0-mlogloss:0.85864\n",
      "[92]\tvalidation_0-mlogloss:0.85896\n",
      "[93]\tvalidation_0-mlogloss:0.85896\n",
      "[94]\tvalidation_0-mlogloss:0.85923\n",
      "[95]\tvalidation_0-mlogloss:0.85939\n",
      "[96]\tvalidation_0-mlogloss:0.85916\n",
      "[97]\tvalidation_0-mlogloss:0.85894\n",
      "[98]\tvalidation_0-mlogloss:0.85922\n",
      "[99]\tvalidation_0-mlogloss:0.85961\n",
      "[100]\tvalidation_0-mlogloss:0.85917\n",
      "[101]\tvalidation_0-mlogloss:0.85994\n",
      "[102]\tvalidation_0-mlogloss:0.86017\n",
      "[103]\tvalidation_0-mlogloss:0.86041\n",
      "[104]\tvalidation_0-mlogloss:0.86027\n",
      "[105]\tvalidation_0-mlogloss:0.86010\n",
      "[106]\tvalidation_0-mlogloss:0.86047\n",
      "[107]\tvalidation_0-mlogloss:0.86065\n",
      "[108]\tvalidation_0-mlogloss:0.86066\n",
      "[109]\tvalidation_0-mlogloss:0.86075\n",
      "[110]\tvalidation_0-mlogloss:0.86105\n",
      "[111]\tvalidation_0-mlogloss:0.86115\n",
      "[112]\tvalidation_0-mlogloss:0.86097\n",
      "[113]\tvalidation_0-mlogloss:0.86075\n",
      "[114]\tvalidation_0-mlogloss:0.86106\n",
      "[115]\tvalidation_0-mlogloss:0.86141\n",
      "[116]\tvalidation_0-mlogloss:0.86164\n",
      "[117]\tvalidation_0-mlogloss:0.86239\n",
      "[118]\tvalidation_0-mlogloss:0.86239\n",
      "[119]\tvalidation_0-mlogloss:0.86241\n",
      "[120]\tvalidation_0-mlogloss:0.86293\n",
      "[121]\tvalidation_0-mlogloss:0.86296\n",
      "[122]\tvalidation_0-mlogloss:0.86253\n",
      "[123]\tvalidation_0-mlogloss:0.86286\n",
      "[124]\tvalidation_0-mlogloss:0.86336\n",
      "[125]\tvalidation_0-mlogloss:0.86393\n",
      "[126]\tvalidation_0-mlogloss:0.86424\n",
      "[127]\tvalidation_0-mlogloss:0.86504\n",
      "[128]\tvalidation_0-mlogloss:0.86523\n",
      "[129]\tvalidation_0-mlogloss:0.86515\n",
      "[130]\tvalidation_0-mlogloss:0.86587\n",
      "[131]\tvalidation_0-mlogloss:0.86621\n",
      "[132]\tvalidation_0-mlogloss:0.86656\n",
      "[133]\tvalidation_0-mlogloss:0.86699\n",
      "[134]\tvalidation_0-mlogloss:0.86742\n",
      "[135]\tvalidation_0-mlogloss:0.86759\n",
      "[136]\tvalidation_0-mlogloss:0.86783\n",
      "[137]\tvalidation_0-mlogloss:0.86772\n",
      "[138]\tvalidation_0-mlogloss:0.86807\n",
      "[139]\tvalidation_0-mlogloss:0.86798\n",
      "[140]\tvalidation_0-mlogloss:0.86875\n",
      "[141]\tvalidation_0-mlogloss:0.86898\n",
      "[142]\tvalidation_0-mlogloss:0.86859\n",
      "[143]\tvalidation_0-mlogloss:0.86891\n",
      "[144]\tvalidation_0-mlogloss:0.86910\n",
      "[145]\tvalidation_0-mlogloss:0.86945\n",
      "[146]\tvalidation_0-mlogloss:0.86998\n",
      "[147]\tvalidation_0-mlogloss:0.87053\n",
      "[148]\tvalidation_0-mlogloss:0.87099\n",
      "[149]\tvalidation_0-mlogloss:0.87108\n",
      "[150]\tvalidation_0-mlogloss:0.87154\n",
      "[151]\tvalidation_0-mlogloss:0.87197\n",
      "[152]\tvalidation_0-mlogloss:0.87231\n",
      "[153]\tvalidation_0-mlogloss:0.87225\n",
      "[154]\tvalidation_0-mlogloss:0.87299\n",
      "[155]\tvalidation_0-mlogloss:0.87331\n",
      "[156]\tvalidation_0-mlogloss:0.87318\n",
      "[157]\tvalidation_0-mlogloss:0.87382\n",
      "[158]\tvalidation_0-mlogloss:0.87425\n",
      "[159]\tvalidation_0-mlogloss:0.87408\n",
      "[160]\tvalidation_0-mlogloss:0.87497\n",
      "[161]\tvalidation_0-mlogloss:0.87515\n",
      "[162]\tvalidation_0-mlogloss:0.87586\n",
      "[163]\tvalidation_0-mlogloss:0.87628\n",
      "[164]\tvalidation_0-mlogloss:0.87645\n",
      "[165]\tvalidation_0-mlogloss:0.87704\n",
      "[166]\tvalidation_0-mlogloss:0.87728\n",
      "[167]\tvalidation_0-mlogloss:0.87798\n",
      "[168]\tvalidation_0-mlogloss:0.87853\n",
      "[169]\tvalidation_0-mlogloss:0.87885\n",
      "[170]\tvalidation_0-mlogloss:0.87962\n",
      "[171]\tvalidation_0-mlogloss:0.87984\n",
      "[172]\tvalidation_0-mlogloss:0.88001\n",
      "[173]\tvalidation_0-mlogloss:0.88056\n",
      "[174]\tvalidation_0-mlogloss:0.88085\n",
      "[175]\tvalidation_0-mlogloss:0.88104\n",
      "[176]\tvalidation_0-mlogloss:0.88142\n",
      "[177]\tvalidation_0-mlogloss:0.88180\n",
      "[178]\tvalidation_0-mlogloss:0.88183\n",
      "[179]\tvalidation_0-mlogloss:0.88248\n",
      "[180]\tvalidation_0-mlogloss:0.88282\n",
      "[181]\tvalidation_0-mlogloss:0.88255\n",
      "[182]\tvalidation_0-mlogloss:0.88295\n",
      "[183]\tvalidation_0-mlogloss:0.88378\n",
      "[184]\tvalidation_0-mlogloss:0.88419\n",
      "[185]\tvalidation_0-mlogloss:0.88487\n",
      "[186]\tvalidation_0-mlogloss:0.88490\n",
      "[187]\tvalidation_0-mlogloss:0.88534\n",
      "[188]\tvalidation_0-mlogloss:0.88575\n",
      "[189]\tvalidation_0-mlogloss:0.88591\n",
      "[190]\tvalidation_0-mlogloss:0.88627\n",
      "[191]\tvalidation_0-mlogloss:0.88675\n",
      "[192]\tvalidation_0-mlogloss:0.88700\n",
      "[193]\tvalidation_0-mlogloss:0.88741\n",
      "[194]\tvalidation_0-mlogloss:0.88777\n",
      "[195]\tvalidation_0-mlogloss:0.88821\n",
      "[196]\tvalidation_0-mlogloss:0.88864\n",
      "[197]\tvalidation_0-mlogloss:0.88911\n",
      "[198]\tvalidation_0-mlogloss:0.88966\n",
      "[199]\tvalidation_0-mlogloss:0.88967\n",
      "[INFO] Training completed!\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[ 14   2   0  18   2  56]\n",
      " [  2  64   0   3   8  60]\n",
      " [  0   0   2   6   0  12]\n",
      " [  5   4   1  98   2 129]\n",
      " [  1   5   0   5  50  56]\n",
      " [ 16  28   2  36  15 839]]\n",
      "\n",
      "Class: 0\n",
      "TP: 14, TN: 1425, FP: 24, FN: 78\n",
      "Accuracy: 0.9338\n",
      "Error Rate: 0.0662\n",
      "Precision: 0.3684\n",
      "Recall / TPR: 0.1522\n",
      "F1 Score: 0.2154\n",
      "Specificity / TNR: 0.9834\n",
      "False Positive Rate (FPR): 0.0166\n",
      "False Negative Rate (FNR): 0.8478\n",
      "\n",
      "Class: 1\n",
      "TP: 64, TN: 1365, FP: 39, FN: 73\n",
      "Accuracy: 0.9273\n",
      "Error Rate: 0.0727\n",
      "Precision: 0.6214\n",
      "Recall / TPR: 0.4672\n",
      "F1 Score: 0.5333\n",
      "Specificity / TNR: 0.9722\n",
      "False Positive Rate (FPR): 0.0278\n",
      "False Negative Rate (FNR): 0.5328\n",
      "\n",
      "Class: 2\n",
      "TP: 2, TN: 1518, FP: 3, FN: 18\n",
      "Accuracy: 0.9864\n",
      "Error Rate: 0.0136\n",
      "Precision: 0.4000\n",
      "Recall / TPR: 0.1000\n",
      "F1 Score: 0.1600\n",
      "Specificity / TNR: 0.9980\n",
      "False Positive Rate (FPR): 0.0020\n",
      "False Negative Rate (FNR): 0.9000\n",
      "\n",
      "Class: 3\n",
      "TP: 98, TN: 1234, FP: 68, FN: 141\n",
      "Accuracy: 0.8644\n",
      "Error Rate: 0.1356\n",
      "Precision: 0.5904\n",
      "Recall / TPR: 0.4100\n",
      "F1 Score: 0.4840\n",
      "Specificity / TNR: 0.9478\n",
      "False Positive Rate (FPR): 0.0522\n",
      "False Negative Rate (FNR): 0.5900\n",
      "\n",
      "Class: 4\n",
      "TP: 50, TN: 1397, FP: 27, FN: 67\n",
      "Accuracy: 0.9390\n",
      "Error Rate: 0.0610\n",
      "Precision: 0.6494\n",
      "Recall / TPR: 0.4274\n",
      "F1 Score: 0.5155\n",
      "Specificity / TNR: 0.9810\n",
      "False Positive Rate (FPR): 0.0190\n",
      "False Negative Rate (FNR): 0.5726\n",
      "\n",
      "Class: 5\n",
      "TP: 839, TN: 292, FP: 313, FN: 97\n",
      "Accuracy: 0.7339\n",
      "Error Rate: 0.2661\n",
      "Precision: 0.7283\n",
      "Recall / TPR: 0.8964\n",
      "F1 Score: 0.8036\n",
      "Specificity / TNR: 0.4826\n",
      "False Positive Rate (FPR): 0.5174\n",
      "False Negative Rate (FNR): 0.1036\n",
      "\n",
      "[INFO] XGBoost evaluation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:00:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:00:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# XGBoost GPU/CPU Training Pipeline\n",
    "# ========================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try GPU-enabled XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"[INFO] XGBoost imported. Will attempt GPU usage if available.\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    raise ImportError(\"XGBoost is not installed. Install it via pip install xgboost.\")\n",
    "\n",
    "# ------------------------\n",
    "# Load dataset\n",
    "# ------------------------\n",
    "\n",
    "print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "X = df.drop(\"tfopwg_disp\", axis=1)\n",
    "y = df[\"tfopwg_disp\"]\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"[INFO] Target classes: {list(le.classes_)}\")\n",
    "\n",
    "# ------------------------\n",
    "# Train-test split (stratified)\n",
    "# ------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"[INFO] Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Scaling optional for XGBoost (not required)\n",
    "# ------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "print(\"[INFO] Features standardized\")\n",
    "\n",
    "# ------------------------\n",
    "# Initialize XGBoost Classifier\n",
    "# ------------------------\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": len(le.classes_),\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 200,\n",
    "    \"verbosity\": 1,\n",
    "    \"tree_method\": \"gpu_hist\" if USE_GPU else \"hist\"\n",
    "}\n",
    "\n",
    "print(f\"[INFO] Training XGBoost with {'GPU' if USE_GPU else 'CPU'}...\")\n",
    "model = xgb.XGBClassifier(**params)\n",
    "model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=True)\n",
    "print(\"[INFO] Training completed!\")\n",
    "\n",
    "# ------------------------\n",
    "# Predictions\n",
    "# ------------------------\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# ------------------------\n",
    "# Confusion Matrix & Metrics\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"\\n===== CONFUSION MATRIX =====\")\n",
    "print(cm)\n",
    "\n",
    "# Multi-class metrics per class\n",
    "classes = le.classes_\n",
    "for idx, cls in enumerate(classes):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {cls}\")\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "    \n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Error_Rate = 1 - Accuracy\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
    "    Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "    FNR = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "    print(f\"Error Rate: {Error_Rate:.4f}\")\n",
    "    print(f\"Precision: {Precision:.4f}\")\n",
    "    print(f\"Recall / TPR: {Recall:.4f}\")\n",
    "    print(f\"F1 Score: {F1:.4f}\")\n",
    "    print(f\"Specificity / TNR: {Specificity:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {FNR:.4f}\")\n",
    "\n",
    "print(\"\\n[INFO] XGBoost evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4e2d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CatBoost imported. Will attempt GPU usage if available.\n",
      "[INFO] Dataset loaded. Shape: (7703, 32)\n",
      "[INFO] Target classes: [0, 1, 2, 3, 4, 5]\n",
      "[INFO] Train shape: (6162, 31), Validation shape: (1541, 31)\n",
      "[INFO] Features standardized\n",
      "[INFO] Training CatBoost with GPU...\n",
      "0:\tlearn: 1.6152467\ttest: 1.6184722\tbest: 1.6184722 (0)\ttotal: 20.2ms\tremaining: 10.1s\n",
      "50:\tlearn: 0.8175552\ttest: 0.8975059\tbest: 0.8975059 (50)\ttotal: 321ms\tremaining: 2.82s\n",
      "100:\tlearn: 0.7193936\ttest: 0.8587612\tbest: 0.8587612 (100)\ttotal: 783ms\tremaining: 3.09s\n",
      "150:\tlearn: 0.6517895\ttest: 0.8454848\tbest: 0.8454223 (148)\ttotal: 1.27s\tremaining: 2.93s\n",
      "200:\tlearn: 0.5982811\ttest: 0.8391417\tbest: 0.8385934 (198)\ttotal: 1.71s\tremaining: 2.54s\n",
      "250:\tlearn: 0.5513472\ttest: 0.8343559\tbest: 0.8339948 (235)\ttotal: 2.17s\tremaining: 2.15s\n",
      "300:\tlearn: 0.5117353\ttest: 0.8292642\tbest: 0.8291505 (290)\ttotal: 2.63s\tremaining: 1.74s\n",
      "350:\tlearn: 0.4754314\ttest: 0.8307571\tbest: 0.8289083 (310)\ttotal: 3.09s\tremaining: 1.31s\n",
      "400:\tlearn: 0.4436721\ttest: 0.8284571\tbest: 0.8280496 (397)\ttotal: 3.5s\tremaining: 864ms\n",
      "450:\tlearn: 0.4114483\ttest: 0.8291927\tbest: 0.8280496 (397)\ttotal: 3.91s\tremaining: 424ms\n",
      "499:\tlearn: 0.3841840\ttest: 0.8301546\tbest: 0.8280496 (397)\ttotal: 4.37s\tremaining: 0us\n",
      "bestTest = 0.8280495873\n",
      "bestIteration = 397\n",
      "Shrink model to first 398 iterations.\n",
      "[INFO] Training completed!\n",
      "\n",
      "===== CONFUSION MATRIX =====\n",
      "[[ 19   3   0  22   0  48]\n",
      " [  3  60   0   2   6  66]\n",
      " [  0   0   2   6   0  12]\n",
      " [  6   6   1 103   1 122]\n",
      " [  1   6   0   2  49  59]\n",
      " [ 15  29   0  38  17 837]]\n",
      "\n",
      "Class: 0\n",
      "TP: 19, TN: 1424, FP: 25, FN: 73\n",
      "Accuracy: 0.9364\n",
      "Error Rate: 0.0636\n",
      "Precision: 0.4318\n",
      "Recall / TPR: 0.2065\n",
      "F1 Score: 0.2794\n",
      "Specificity / TNR: 0.9827\n",
      "False Positive Rate (FPR): 0.0173\n",
      "False Negative Rate (FNR): 0.7935\n",
      "\n",
      "Class: 1\n",
      "TP: 60, TN: 1360, FP: 44, FN: 77\n",
      "Accuracy: 0.9215\n",
      "Error Rate: 0.0785\n",
      "Precision: 0.5769\n",
      "Recall / TPR: 0.4380\n",
      "F1 Score: 0.4979\n",
      "Specificity / TNR: 0.9687\n",
      "False Positive Rate (FPR): 0.0313\n",
      "False Negative Rate (FNR): 0.5620\n",
      "\n",
      "Class: 2\n",
      "TP: 2, TN: 1520, FP: 1, FN: 18\n",
      "Accuracy: 0.9877\n",
      "Error Rate: 0.0123\n",
      "Precision: 0.6667\n",
      "Recall / TPR: 0.1000\n",
      "F1 Score: 0.1739\n",
      "Specificity / TNR: 0.9993\n",
      "False Positive Rate (FPR): 0.0007\n",
      "False Negative Rate (FNR): 0.9000\n",
      "\n",
      "Class: 3\n",
      "TP: 103, TN: 1232, FP: 70, FN: 136\n",
      "Accuracy: 0.8663\n",
      "Error Rate: 0.1337\n",
      "Precision: 0.5954\n",
      "Recall / TPR: 0.4310\n",
      "F1 Score: 0.5000\n",
      "Specificity / TNR: 0.9462\n",
      "False Positive Rate (FPR): 0.0538\n",
      "False Negative Rate (FNR): 0.5690\n",
      "\n",
      "Class: 4\n",
      "TP: 49, TN: 1400, FP: 24, FN: 68\n",
      "Accuracy: 0.9403\n",
      "Error Rate: 0.0597\n",
      "Precision: 0.6712\n",
      "Recall / TPR: 0.4188\n",
      "F1 Score: 0.5158\n",
      "Specificity / TNR: 0.9831\n",
      "False Positive Rate (FPR): 0.0169\n",
      "False Negative Rate (FNR): 0.5812\n",
      "\n",
      "Class: 5\n",
      "TP: 837, TN: 298, FP: 307, FN: 99\n",
      "Accuracy: 0.7365\n",
      "Error Rate: 0.2635\n",
      "Precision: 0.7316\n",
      "Recall / TPR: 0.8942\n",
      "F1 Score: 0.8048\n",
      "Specificity / TNR: 0.4926\n",
      "False Positive Rate (FPR): 0.5074\n",
      "False Negative Rate (FNR): 0.1058\n",
      "\n",
      "[INFO] CatBoost evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# CatBoost GPU/CPU Training Pipeline\n",
    "# ========================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try GPU-enabled CatBoost\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    print(\"[INFO] CatBoost imported. Will attempt GPU usage if available.\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    raise ImportError(\"CatBoost is not installed. Install it via pip install catboost.\")\n",
    "\n",
    "# ------------------------\n",
    "# Load dataset\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"tess_toi_ml_ready.csv\")\n",
    "print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------\n",
    "X = df.drop(\"tfopwg_disp\", axis=1)\n",
    "y = df[\"tfopwg_disp\"]\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"[INFO] Target classes: {list(le.classes_)}\")\n",
    "\n",
    "# ------------------------\n",
    "# Train-test split (stratified)\n",
    "# ------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"[INFO] Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n",
    "\n",
    "# ------------------------\n",
    "# Scaling optional for CatBoost\n",
    "# ------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "print(\"[INFO] Features standardized\")\n",
    "\n",
    "# ------------------------\n",
    "# Initialize CatBoost Classifier\n",
    "# ------------------------\n",
    "catboost_params = {\n",
    "    \"iterations\": 500,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"depth\": 6,\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"verbose\": 50,\n",
    "    \"task_type\": \"GPU\" if USE_GPU else \"CPU\"\n",
    "}\n",
    "\n",
    "print(f\"[INFO] Training CatBoost with {'GPU' if USE_GPU else 'CPU'}...\")\n",
    "model = CatBoostClassifier(**catboost_params)\n",
    "model.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val))\n",
    "print(\"[INFO] Training completed!\")\n",
    "\n",
    "# ------------------------\n",
    "# Predictions\n",
    "# ------------------------\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten().astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# Confusion Matrix & Metrics\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"\\n===== CONFUSION MATRIX =====\")\n",
    "print(cm)\n",
    "\n",
    "# Multi-class metrics per class\n",
    "classes = le.classes_\n",
    "for idx, cls in enumerate(classes):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {cls}\")\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "    \n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Error_Rate = 1 - Accuracy\n",
    "    Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
    "    Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "    FNR = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "    \n",
    "    print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "    print(f\"Error Rate: {Error_Rate:.4f}\")\n",
    "    print(f\"Precision: {Precision:.4f}\")\n",
    "    print(f\"Recall / TPR: {Recall:.4f}\")\n",
    "    print(f\"F1 Score: {F1:.4f}\")\n",
    "    print(f\"Specificity / TNR: {Specificity:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {FNR:.4f}\")\n",
    "\n",
    "print(\"\\n[INFO] CatBoost evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d59fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM training...\n",
      "Training SVM classifier...\n",
      "Training completed.\n",
      "Making predictions...\n",
      "Predictions done.\n",
      "\n",
      "===== SVM CLASSIFICATION METRICS =====\n",
      "Confusion Matrix:\n",
      "[[  0   1   0   9   1  35]\n",
      " [  1  15   0   0   0  53]\n",
      " [  0   0   0   1   0   9]\n",
      " [  0   2   0  39   0  79]\n",
      " [  0   1   0   0   5  52]\n",
      " [  0   9   0  15   3 441]]\n",
      "TP: 500, TN: -271, FP: 271, FN: 271\n",
      "Accuracy: 0.2970\n",
      "Error Rate: 0.7030\n",
      "Precision: 0.6485\n",
      "Recall / TPR: 0.6485\n",
      "F1 Score: 0.6485\n",
      "Specificity / TNR: 0.0000\n",
      "FPR: 0.0000\n",
      "FNR: 0.3515\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import cupy as cp  # We'll use cp arrays if needed for compatibility\n",
    "\n",
    "# Assuming X_train, X_val, y_train, y_val are ready (CPU)\n",
    "print(\"Starting SVM training...\")\n",
    "\n",
    "# Feature Scaling is important for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "y_pred = svm_model.predict(X_val_scaled)\n",
    "print(\"Predictions done.\")\n",
    "\n",
    "# Metrics calculation\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "TP = sum([cm[i,i] for i in range(cm.shape[0])])\n",
    "FP = sum([cm[:,i].sum() - cm[i,i] for i in range(cm.shape[0])])\n",
    "FN = sum([cm[i,:].sum() - cm[i,i] for i in range(cm.shape[0])])\n",
    "TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Error_Rate = 1 - Accuracy\n",
    "Precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "Recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "F1 = 2 * Precision * Recall / (Precision + Recall) if (Precision + Recall) != 0 else 0\n",
    "Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "FNR = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\n===== SVM CLASSIFICATION METRICS =====\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "print(f\"Error Rate: {Error_Rate:.4f}\")\n",
    "print(f\"Precision: {Precision:.4f}\")\n",
    "print(f\"Recall / TPR: {Recall:.4f}\")\n",
    "print(f\"F1 Score: {F1:.4f}\")\n",
    "print(f\"Specificity / TNR: {Specificity:.4f}\")\n",
    "print(f\"FPR: {FPR:.4f}\")\n",
    "print(f\"FNR: {FNR:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
