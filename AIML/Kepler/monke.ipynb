{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94d2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try optional CuPy (GPU arrays)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    HAS_CUPY = True\n",
    "except ImportError:\n",
    "    HAS_CUPY = False\n",
    "    cp = None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helper functions\n",
    "# -----------------------\n",
    "def extract_id_from_filename(filename: str):\n",
    "    \"\"\"Extract kepid/KIC ID from filename like lightcurve_0_KIC6020753.csv\"\"\"\n",
    "    bn = os.path.basename(filename).replace(\".csv\", \"\")\n",
    "    return bn.split(\"_\")[-1]  # e.g. KIC6020753\n",
    "\n",
    "\n",
    "def normalize_and_resample_gpu(flux, target_length=2000):\n",
    "    flux = cp.asarray(flux, dtype=cp.float32)\n",
    "\n",
    "    if flux.size < 10 or cp.all(cp.isnan(flux)):\n",
    "        raise ValueError(\"Flux array too short or all NaNs\")\n",
    "\n",
    "    # Clean bad values\n",
    "    flux = cp.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    flux = cp.clip(flux, -5, 5)\n",
    "\n",
    "    # Normalize\n",
    "    med = cp.nanmedian(flux)\n",
    "    if not cp.isfinite(med) or med == 0:\n",
    "        med = 1.0\n",
    "    flux = (flux / med) - 1.0\n",
    "\n",
    "    # Resample to fixed length\n",
    "    x_old = cp.arange(flux.size, dtype=cp.float32)\n",
    "    x_new = cp.linspace(0, flux.size - 1, target_length, dtype=cp.float32)\n",
    "    flux_res = cp.interp(x_new, x_old, flux)\n",
    "\n",
    "    return cp.asnumpy(flux_res)\n",
    "\n",
    "\n",
    "\n",
    "def process_file(file_path, catalog_df, target_length, tabular_cols, use_gpu=False):\n",
    "    \"\"\"Process single lightcurve file and join with catalog\"\"\"\n",
    "    try:\n",
    "        # Use Polars instead of pandas for speed\n",
    "        df = pl.read_csv(file_path)\n",
    "        if \"flux\" not in df.columns:\n",
    "            print(f\"âš ï¸ Skipping {file_path}: no 'flux' column\")\n",
    "            return None\n",
    "\n",
    "        flux = df[\"flux\"].to_numpy()\n",
    "        token = extract_id_from_filename(file_path)\n",
    "\n",
    "        # Match with catalog\n",
    "        kepid_digits = re.sub(r\"\\D\", \"\", token)\n",
    "        match = catalog_df[catalog_df[\"kepid\"].astype(str).str.contains(kepid_digits)]\n",
    "        if len(match) == 0:\n",
    "            print(f\"âš ï¸ No catalog match for {file_path} (token={token})\")\n",
    "            return None\n",
    "        row = match.iloc[0]\n",
    "\n",
    "        # Normalize + resample\n",
    "        if use_gpu and HAS_CUPY:\n",
    "            flux_res = normalize_and_resample_gpu(flux, target_length)\n",
    "        else:\n",
    "            # CPU fallback\n",
    "            flux = flux.astype(np.float32)\n",
    "            med = np.nanmedian(flux) if np.isfinite(np.nanmedian(flux)) else 1.0\n",
    "            if med == 0: med = 1.0\n",
    "            flux = (flux / med) - 1.0\n",
    "            x_old = np.arange(len(flux))\n",
    "            x_new = np.linspace(0, len(flux) - 1, target_length)\n",
    "            flux_res = np.interp(x_new, x_old, flux).astype(np.float32)\n",
    "\n",
    "        # Collect\n",
    "        label = int(row[\"target_encoded\"])\n",
    "        tabular = row[tabular_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        return flux_res, tabular, label, row[\"rowid\"], row[\"kepid\"], row[\"kepoi_name\"], os.path.basename(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Main Builder\n",
    "# -----------------------\n",
    "def build_dataset(catalog_path=\"kepler_with_lightcurves.csv\",\n",
    "                  lc_folder=\"folder\",\n",
    "                  out_dir=\"dataset_out\",\n",
    "                  target_length=2000,\n",
    "                  workers=8,\n",
    "                  use_gpu=True):\n",
    "\n",
    "    print(\"ðŸ“‚ Loading catalog...\")\n",
    "    import pandas as pd\n",
    "    catalog_df = pd.read_csv(catalog_path)\n",
    "    print(f\"âœ… Catalog loaded with {len(catalog_df)} rows\")\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    exclude_cols = set([\"rowid\", \"kepid\", \"kepoi_name\", \"koi_disposition\", \"target_encoded\"])\n",
    "    tabular_cols = [c for c in catalog_df.columns if c not in exclude_cols and \n",
    "                    np.issubdtype(catalog_df[c].dtype, np.number)]\n",
    "    print(f\"ðŸ“Š Using {len(tabular_cols)} tabular features\")\n",
    "\n",
    "    files = sorted(Path(lc_folder).glob(\"*.csv\"))\n",
    "    print(f\"ðŸ” Found {len(files)} lightcurve files\")\n",
    "\n",
    "    X_lc, X_tab, y, meta = [], [], [], []\n",
    "    error_count = 0\n",
    "\n",
    "    print(f\"ðŸš€ Starting sequential processing (GPU={use_gpu and HAS_CUPY})...\")\n",
    "    for f in tqdm(files, desc=\"Processing\"):\n",
    "        res = process_file(f, catalog_df, target_length, tabular_cols, use_gpu)\n",
    "        if res is None:\n",
    "            error_count += 1\n",
    "            continue\n",
    "        flux_res, tabular, label, rowid, kepid, kepoi_name, fname = res\n",
    "        X_lc.append(flux_res)\n",
    "        X_tab.append(tabular)\n",
    "        y.append(label)\n",
    "        meta.append([rowid, kepid, kepoi_name, fname])\n",
    "\n",
    "    print(f\"âœ… Finished. Successful: {len(X_lc)}, Errors: {error_count}\")\n",
    "\n",
    "    # Convert to arrays\n",
    "    print(\"ðŸ“¦ Stacking arrays...\")\n",
    "    X_lc = np.stack(X_lc).astype(np.float32)\n",
    "    X_tab = np.stack(X_tab).astype(np.float32)\n",
    "    y = np.array(y, dtype=np.int8)\n",
    "\n",
    "    # Save\n",
    "    print(f\"ðŸ’¾ Saving to {out_dir} ...\")\n",
    "    np.save(out_dir / \"X_lightcurves.npy\", X_lc)\n",
    "    np.save(out_dir / \"X_catalog.npy\", X_tab)\n",
    "    np.save(out_dir / \"y.npy\", y)\n",
    "\n",
    "    import pandas as pd\n",
    "    meta_df = pd.DataFrame(meta, columns=[\"rowid\", \"kepid\", \"kepoi_name\", \"filename\"])\n",
    "    meta_df.to_csv(out_dir / \"meta.csv\", index=False)\n",
    "\n",
    "    print(\"ðŸŽ‰ Done.\")\n",
    "    print(f\"Shapes: X_lightcurves={X_lc.shape}, X_catalog={X_tab.shape}, y={y.shape}\")\n",
    "\n",
    "    return X_lc, X_tab, y, meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338fbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading catalog...\n",
      "âœ… Catalog loaded with 2165 rows\n",
      "ðŸ“Š Using 51 tabular features\n",
      "ðŸ” Found 5019 lightcurve files\n",
      "ðŸš€ Starting sequential processing (GPU=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2494/5019 [00:15<00:16, 152.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error processing small\\lightcurve_4230_KIC3749365.csv: Flux array too short or all NaNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5019/5019 [00:35<00:00, 142.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished. Successful: 5018, Errors: 1\n",
      "ðŸ“¦ Stacking arrays...\n",
      "ðŸ’¾ Saving to dataset_out ...\n",
      "ðŸŽ‰ Done.\n",
      "Shapes: X_lightcurves=(5018, 2000), X_catalog=(5018, 51), y=(5018,)\n"
     ]
    }
   ],
   "source": [
    "X_lc, X_tab, y, meta = build_dataset(\n",
    "    catalog_path=\"kepler_with_lightcurves.csv\",\n",
    "    lc_folder=\"small\",\n",
    "    out_dir=\"dataset_out\",\n",
    "    target_length=2000,\n",
    "    workers=12,\n",
    "    use_gpu=True  # set True if you want CuPy arrays\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75689c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading dataset (GPU)...\n",
      "âœ… Shapes: (5018, 2000) (5018, 51) (5018,)\n",
      "Epoch 1/10 - Loss: nan\n",
      "Epoch 2/10 - Loss: nan\n",
      "Epoch 3/10 - Loss: nan\n",
      "Epoch 4/10 - Loss: nan\n",
      "Epoch 5/10 - Loss: nan\n",
      "Epoch 6/10 - Loss: nan\n",
      "Epoch 7/10 - Loss: nan\n",
      "Epoch 8/10 - Loss: nan\n",
      "Epoch 9/10 - Loss: nan\n",
      "Epoch 10/10 - Loss: nan\n",
      "\n",
      "ðŸ“Š Confusion Matrix (rows=true, cols=pred):\n",
      "[[  7   0   0]\n",
      " [992   0   0]\n",
      " [  5   0   0]]\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CANDIDATE       0.01      1.00      0.01         7\n",
      "     CONFIRMED       0.00      0.00      0.00       992\n",
      "FALSE POSITIVE       0.00      0.00      0.00         5\n",
      "\n",
      "      accuracy                           0.01      1004\n",
      "     macro avg       0.00      0.33      0.00      1004\n",
      "  weighted avg       0.00      0.01      0.00      1004\n",
      "\n",
      "\n",
      "âœ… Overall Accuracy: 0.0070\n",
      "âŒ Error Rate: 0.9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# ======================\n",
    "# Load Preprocessed Data\n",
    "# ======================\n",
    "print(\"ðŸ“‚ Loading dataset (GPU)...\")\n",
    "X_lc = cp.load(\"dataset_out/X_lightcurves.npy\")\n",
    "X_tab = cp.load(\"dataset_out/X_catalog.npy\")\n",
    "y = cp.load(\"dataset_out/y.npy\")\n",
    "\n",
    "print(\"âœ… Shapes:\", X_lc.shape, X_tab.shape, y.shape)\n",
    "\n",
    "# Convert to torch tensors (GPU)\n",
    "X_lc_t = torch.tensor(cp.asnumpy(X_lc), dtype=torch.float32, device=\"cuda\")\n",
    "X_tab_t = torch.tensor(cp.asnumpy(X_tab), dtype=torch.float32, device=\"cuda\")\n",
    "y_t = torch.tensor(cp.asnumpy(y), dtype=torch.long, device=\"cuda\")\n",
    "\n",
    "# ======================\n",
    "# Dataset + Dataloader\n",
    "# ======================\n",
    "dataset = TensorDataset(X_lc_t, X_tab_t, y_t)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# ======================\n",
    "# Model Definition\n",
    "# ======================\n",
    "class DualInputModel(nn.Module):\n",
    "    def __init__(self, lc_length, tab_features, num_classes=3):\n",
    "        super().__init__()\n",
    "        # Lightcurve branch (1D CNN)\n",
    "        self.lc_branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(16)\n",
    "        )\n",
    "        self.lc_fc = nn.Linear(64 * 16, 128)\n",
    "\n",
    "        # Tabular branch\n",
    "        self.tab_branch = nn.Sequential(\n",
    "            nn.Linear(tab_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combined\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(128 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, lc, tab):\n",
    "        lc = lc.unsqueeze(1)  # (batch, 1, length)\n",
    "        lc = self.lc_branch(lc)\n",
    "        lc = lc.view(lc.size(0), -1)\n",
    "        lc = self.lc_fc(lc)\n",
    "\n",
    "        tab = self.tab_branch(tab)\n",
    "\n",
    "        combined = torch.cat([lc, tab], dim=1)\n",
    "        out = self.combined_fc(combined)\n",
    "        return out\n",
    "\n",
    "# ======================\n",
    "# Training Setup\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DualInputModel(lc_length=X_lc.shape[1], tab_features=X_tab.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# Training Loop\n",
    "# ======================\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for lc_batch, tab_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(lc_batch, tab_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ======================\n",
    "# Evaluation\n",
    "# ======================\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for lc_batch, tab_batch, y_batch in val_loader:\n",
    "        out = model(lc_batch, tab_batch)\n",
    "        preds = out.argmax(dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "\n",
    "all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2])  # CANDIDATE=0, CONFIRMED=1, FALSE POSITIVE=2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2])\n",
    "print(\"\\nðŸ“Š Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "# Detailed metrics per class\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"CANDIDATE\",\"CONFIRMED\",\"FALSE POSITIVE\"]))\n",
    "\n",
    "# Manual summary (macro-averaged)\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"\\nâœ… Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âŒ Error Rate: {error_rate:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
