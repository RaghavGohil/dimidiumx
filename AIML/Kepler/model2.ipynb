{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93af330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading train_ready.csv ...\n",
      "âœ… Tabular data shape: (7651, 117)\n",
      "ðŸš€ Training XGBoost (GPU) ...\n",
      "[0]\ttest-logloss:0.47778\n",
      "[1]\ttest-logloss:0.35735\n",
      "[2]\ttest-logloss:0.28115\n",
      "[3]\ttest-logloss:0.23098\n",
      "[4]\ttest-logloss:0.19511\n",
      "[5]\ttest-logloss:0.17033\n",
      "[6]\ttest-logloss:0.15121\n",
      "[7]\ttest-logloss:0.13867\n",
      "[8]\ttest-logloss:0.12907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:40:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:40:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\ttest-logloss:0.12312\n",
      "[10]\ttest-logloss:0.11719\n",
      "[11]\ttest-logloss:0.11301\n",
      "[12]\ttest-logloss:0.10921\n",
      "[13]\ttest-logloss:0.10649\n",
      "[14]\ttest-logloss:0.10458\n",
      "[15]\ttest-logloss:0.10332\n",
      "[16]\ttest-logloss:0.10279\n",
      "[17]\ttest-logloss:0.10224\n",
      "[18]\ttest-logloss:0.10128\n",
      "[19]\ttest-logloss:0.10012\n",
      "[20]\ttest-logloss:0.09988\n",
      "[21]\ttest-logloss:0.09947\n",
      "[22]\ttest-logloss:0.09942\n",
      "[23]\ttest-logloss:0.09964\n",
      "[24]\ttest-logloss:0.09947\n",
      "[25]\ttest-logloss:0.09956\n",
      "[26]\ttest-logloss:0.09924\n",
      "[27]\ttest-logloss:0.09879\n",
      "[28]\ttest-logloss:0.09936\n",
      "[29]\ttest-logloss:0.09997\n",
      "[30]\ttest-logloss:0.10102\n",
      "[31]\ttest-logloss:0.10098\n",
      "[32]\ttest-logloss:0.10119\n",
      "[33]\ttest-logloss:0.10093\n",
      "[34]\ttest-logloss:0.10090\n",
      "[35]\ttest-logloss:0.10103\n",
      "[36]\ttest-logloss:0.10120\n",
      "[37]\ttest-logloss:0.10127\n",
      "[38]\ttest-logloss:0.10241\n",
      "[39]\ttest-logloss:0.10206\n",
      "[40]\ttest-logloss:0.10226\n",
      "[41]\ttest-logloss:0.10223\n",
      "[42]\ttest-logloss:0.10226\n",
      "[43]\ttest-logloss:0.10234\n",
      "[44]\ttest-logloss:0.10266\n",
      "[45]\ttest-logloss:0.10250\n",
      "[46]\ttest-logloss:0.10253\n",
      "[47]\ttest-logloss:0.10211\n",
      "[48]\ttest-logloss:0.10267\n",
      "[49]\ttest-logloss:0.10254\n",
      "[50]\ttest-logloss:0.10310\n",
      "[51]\ttest-logloss:0.10346\n",
      "[52]\ttest-logloss:0.10401\n",
      "[53]\ttest-logloss:0.10417\n",
      "[54]\ttest-logloss:0.10472\n",
      "[55]\ttest-logloss:0.10465\n",
      "[56]\ttest-logloss:0.10508\n",
      "[57]\ttest-logloss:0.10538\n",
      "[58]\ttest-logloss:0.10519\n",
      "[59]\ttest-logloss:0.10635\n",
      "[60]\ttest-logloss:0.10688\n",
      "[61]\ttest-logloss:0.10728\n",
      "[62]\ttest-logloss:0.10752\n",
      "[63]\ttest-logloss:0.10765\n",
      "[64]\ttest-logloss:0.10789\n",
      "[65]\ttest-logloss:0.10870\n",
      "[66]\ttest-logloss:0.10935\n",
      "[67]\ttest-logloss:0.10884\n",
      "[68]\ttest-logloss:0.10890\n",
      "[69]\ttest-logloss:0.10950\n",
      "[70]\ttest-logloss:0.10953\n",
      "[71]\ttest-logloss:0.11013\n",
      "[72]\ttest-logloss:0.11047\n",
      "[73]\ttest-logloss:0.11074\n",
      "[74]\ttest-logloss:0.11089\n",
      "[75]\ttest-logloss:0.11103\n",
      "[76]\ttest-logloss:0.11051\n",
      "[77]\ttest-logloss:0.11130\n",
      "[78]\ttest-logloss:0.11157\n",
      "[79]\ttest-logloss:0.11163\n",
      "[80]\ttest-logloss:0.11195\n",
      "[81]\ttest-logloss:0.11193\n",
      "[82]\ttest-logloss:0.11190\n",
      "[83]\ttest-logloss:0.11237\n",
      "[84]\ttest-logloss:0.11306\n",
      "[85]\ttest-logloss:0.11352\n",
      "[86]\ttest-logloss:0.11405\n",
      "[87]\ttest-logloss:0.11461\n",
      "[88]\ttest-logloss:0.11456\n",
      "[89]\ttest-logloss:0.11497\n",
      "[90]\ttest-logloss:0.11501\n",
      "[91]\ttest-logloss:0.11500\n",
      "[92]\ttest-logloss:0.11452\n",
      "[93]\ttest-logloss:0.11476\n",
      "[94]\ttest-logloss:0.11482\n",
      "[95]\ttest-logloss:0.11498\n",
      "[96]\ttest-logloss:0.11510\n",
      "[97]\ttest-logloss:0.11523\n",
      "[98]\ttest-logloss:0.11546\n",
      "[99]\ttest-logloss:0.11565\n",
      "[100]\ttest-logloss:0.11626\n",
      "[101]\ttest-logloss:0.11626\n",
      "[102]\ttest-logloss:0.11639\n",
      "[103]\ttest-logloss:0.11636\n",
      "[104]\ttest-logloss:0.11640\n",
      "[105]\ttest-logloss:0.11670\n",
      "[106]\ttest-logloss:0.11651\n",
      "[107]\ttest-logloss:0.11660\n",
      "[108]\ttest-logloss:0.11666\n",
      "[109]\ttest-logloss:0.11736\n",
      "[110]\ttest-logloss:0.11732\n",
      "[111]\ttest-logloss:0.11719\n",
      "[112]\ttest-logloss:0.11703\n",
      "[113]\ttest-logloss:0.11753\n",
      "[114]\ttest-logloss:0.11753\n",
      "[115]\ttest-logloss:0.11767\n",
      "[116]\ttest-logloss:0.11751\n",
      "[117]\ttest-logloss:0.11776\n",
      "[118]\ttest-logloss:0.11780\n",
      "[119]\ttest-logloss:0.11795\n",
      "[120]\ttest-logloss:0.11806\n",
      "[121]\ttest-logloss:0.11815\n",
      "[122]\ttest-logloss:0.11885\n",
      "[123]\ttest-logloss:0.11918\n",
      "[124]\ttest-logloss:0.11970\n",
      "[125]\ttest-logloss:0.11996\n",
      "[126]\ttest-logloss:0.12047\n",
      "[127]\ttest-logloss:0.12078\n",
      "[128]\ttest-logloss:0.12083\n",
      "[129]\ttest-logloss:0.12165\n",
      "[130]\ttest-logloss:0.12207\n",
      "[131]\ttest-logloss:0.12175\n",
      "[132]\ttest-logloss:0.12189\n",
      "[133]\ttest-logloss:0.12178\n",
      "[134]\ttest-logloss:0.12213\n",
      "[135]\ttest-logloss:0.12217\n",
      "[136]\ttest-logloss:0.12233\n",
      "[137]\ttest-logloss:0.12234\n",
      "[138]\ttest-logloss:0.12246\n",
      "[139]\ttest-logloss:0.12245\n",
      "[140]\ttest-logloss:0.12250\n",
      "[141]\ttest-logloss:0.12307\n",
      "[142]\ttest-logloss:0.12307\n",
      "[143]\ttest-logloss:0.12312\n",
      "[144]\ttest-logloss:0.12319\n",
      "[145]\ttest-logloss:0.12331\n",
      "[146]\ttest-logloss:0.12353\n",
      "[147]\ttest-logloss:0.12352\n",
      "[148]\ttest-logloss:0.12363\n",
      "[149]\ttest-logloss:0.12359\n",
      "[150]\ttest-logloss:0.12385\n",
      "[151]\ttest-logloss:0.12406\n",
      "[152]\ttest-logloss:0.12449\n",
      "[153]\ttest-logloss:0.12465\n",
      "[154]\ttest-logloss:0.12489\n",
      "[155]\ttest-logloss:0.12500\n",
      "[156]\ttest-logloss:0.12526\n",
      "[157]\ttest-logloss:0.12482\n",
      "[158]\ttest-logloss:0.12488\n",
      "[159]\ttest-logloss:0.12500\n",
      "[160]\ttest-logloss:0.12539\n",
      "[161]\ttest-logloss:0.12538\n",
      "[162]\ttest-logloss:0.12576\n",
      "[163]\ttest-logloss:0.12575\n",
      "[164]\ttest-logloss:0.12572\n",
      "[165]\ttest-logloss:0.12566\n",
      "[166]\ttest-logloss:0.12589\n",
      "[167]\ttest-logloss:0.12623\n",
      "[168]\ttest-logloss:0.12646\n",
      "[169]\ttest-logloss:0.12647\n",
      "[170]\ttest-logloss:0.12665\n",
      "[171]\ttest-logloss:0.12665\n",
      "[172]\ttest-logloss:0.12692\n",
      "[173]\ttest-logloss:0.12715\n",
      "[174]\ttest-logloss:0.12737\n",
      "[175]\ttest-logloss:0.12758\n",
      "[176]\ttest-logloss:0.12766\n",
      "[177]\ttest-logloss:0.12779\n",
      "[178]\ttest-logloss:0.12768\n",
      "[179]\ttest-logloss:0.12794\n",
      "[180]\ttest-logloss:0.12805\n",
      "[181]\ttest-logloss:0.12803\n",
      "[182]\ttest-logloss:0.12798\n",
      "[183]\ttest-logloss:0.12804\n",
      "[184]\ttest-logloss:0.12823\n",
      "[185]\ttest-logloss:0.12844\n",
      "[186]\ttest-logloss:0.12872\n",
      "[187]\ttest-logloss:0.12897\n",
      "[188]\ttest-logloss:0.12891\n",
      "[189]\ttest-logloss:0.12892\n",
      "[190]\ttest-logloss:0.12900\n",
      "[191]\ttest-logloss:0.12902\n",
      "[192]\ttest-logloss:0.12889\n",
      "[193]\ttest-logloss:0.12876\n",
      "[194]\ttest-logloss:0.12885\n",
      "[195]\ttest-logloss:0.12881\n",
      "[196]\ttest-logloss:0.12879\n",
      "[197]\ttest-logloss:0.12865\n",
      "[198]\ttest-logloss:0.12879\n",
      "[199]\ttest-logloss:0.12901\n",
      "ðŸ“Š XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1364\n",
      "           1       0.93      0.95      0.94       549\n",
      "\n",
      "    accuracy                           0.97      1913\n",
      "   macro avg       0.96      0.96      0.96      1913\n",
      "weighted avg       0.97      0.97      0.97      1913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:40:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load & preprocess train_ready.csv\n",
    "# =====================================================\n",
    "print(\"ðŸ“‚ Loading train_ready.csv ...\")\n",
    "df = pd.read_csv(\"train_ready.csv\")\n",
    "\n",
    "# Encode categorical/text columns\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df.drop([\"target\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Tabular data shape:\", X_train.shape)\n",
    "\n",
    "# =====================================================\n",
    "# 2. XGBoost GPU model\n",
    "# =====================================================\n",
    "print(\"ðŸš€ Training XGBoost (GPU) ...\")\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"scale_pos_weight\": len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=200, evals=[(dtest, \"test\")])\n",
    "preds_xgb = (bst.predict(dtest) > 0.5).astype(int)\n",
    "\n",
    "print(\"ðŸ“Š XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, preds_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68468b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training RandomForest...\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "[[1336   28]\n",
      " [  52  497]]\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9625    0.9795    0.9709      1364\n",
      "           1     0.9467    0.9053    0.9255       549\n",
      "\n",
      "    accuracy                         0.9582      1913\n",
      "   macro avg     0.9546    0.9424    0.9482      1913\n",
      "weighted avg     0.9580    0.9582    0.9579      1913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ======================\n",
    "# Load Dataset\n",
    "# ======================\n",
    "df = pd.read_csv(\"train_ready.csv\")\n",
    "\n",
    "# Drop columns that are identifiers or comments\n",
    "drop_cols = [\"rowid\", \"kepid\", \"kepoi_name\", \"koi_disposition\", \n",
    "             \"koi_vet_date\", \"koi_comment\"]  # adjust if needed\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separate target\n",
    "y = df[\"target\"]\n",
    "X = df.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "\n",
    "# ======================\n",
    "# Train/Val Split\n",
    "# ======================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# RandomForest Model\n",
    "# ======================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training RandomForest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ======================\n",
    "# Evaluation\n",
    "# ======================\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12f8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class KeplerDataset(Dataset):\n",
    "    def __init__(self, csv_file, lightcurve_dir, seq_len=2000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to train_ready.csv\n",
    "            lightcurve_dir (str): Directory with lightcurve CSVs\n",
    "            seq_len (int): Fixed sequence length for lightcurves\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.lightcurve_dir = lightcurve_dir\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Drop text/date fields\n",
    "        drop_cols = [\"rowid\", \"kepid\", \"kepoi_name\", \"koi_disposition\", \n",
    "                     \"koi_vet_date\", \"koi_comment\"]\n",
    "        self.df = self.df.drop(columns=[c for c in drop_cols if c in self.df.columns], errors=\"ignore\")\n",
    "\n",
    "        # Encode categoricals\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        for col in self.df.select_dtypes(include=[\"object\"]).columns:\n",
    "            le = LabelEncoder()\n",
    "            self.df[col] = le.fit_transform(self.df[col].astype(str))\n",
    "\n",
    "        # Split features/target\n",
    "        self.y = torch.tensor(self.df[\"target\"].values, dtype=torch.float32)\n",
    "        self.X = self.df.drop(columns=[\"target\"], errors=\"ignore\").values.astype(np.float32)\n",
    "\n",
    "        self.kepids = self.df[\"kepid\"].values if \"kepid\" in self.df.columns else np.arange(len(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tabular features\n",
    "        tabular = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "\n",
    "        # Try to load lightcurve\n",
    "        kepid = self.kepids[idx]\n",
    "        lc_file = os.path.join(self.lightcurve_dir, f\"lightcurve_{idx}_KIC{kepid}.csv\")\n",
    "        if os.path.exists(lc_file):\n",
    "            lc = pd.read_csv(lc_file)\n",
    "            flux = lc[\"flux\"].values.astype(np.float32)\n",
    "\n",
    "            # Pad or truncate\n",
    "            if len(flux) < self.seq_len:\n",
    "                pad = np.zeros(self.seq_len, dtype=np.float32)\n",
    "                pad[:len(flux)] = flux\n",
    "                flux = pad\n",
    "            else:\n",
    "                flux = flux[:self.seq_len]\n",
    "        else:\n",
    "            # Missing LC â†’ return zeros\n",
    "            flux = np.zeros(self.seq_len, dtype=np.float32)\n",
    "\n",
    "        flux = torch.tensor(flux, dtype=torch.float32).unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "        return flux, tabular, self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b164c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualBranchNN(nn.Module):\n",
    "    def __init__(self, tabular_dim, seq_len=2000, num_classes=1):\n",
    "        super(DualBranchNN, self).__init__()\n",
    "\n",
    "        # Lightcurve CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(32)\n",
    "        )\n",
    "        self.cnn_fc = nn.Linear(64 * 32, 128)\n",
    "\n",
    "        # Tabular MLP branch\n",
    "        self.mlp_branch = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, flux, tabular):\n",
    "        cnn_out = self.cnn_branch(flux)\n",
    "        cnn_out = cnn_out.view(cnn_out.size(0), -1)\n",
    "        cnn_out = self.cnn_fc(cnn_out)\n",
    "\n",
    "        mlp_out = self.mlp_branch(tabular)\n",
    "\n",
    "        combined = torch.cat((cnn_out, mlp_out), dim=1)\n",
    "        out = self.fusion(combined)\n",
    "\n",
    "        return torch.sigmoid(out).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00515a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: nan | Val Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: nan | Val Loss: nan\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "[[1357    0]\n",
      " [ 556    0]]\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7094    1.0000    0.8300      1357\n",
      "         1.0     0.0000    0.0000    0.0000       556\n",
      "\n",
      "    accuracy                         0.7094      1913\n",
      "   macro avg     0.3547    0.5000    0.4150      1913\n",
      "weighted avg     0.5032    0.7094    0.5887      1913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\GPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====================\n",
    "# Dataset & DataLoader\n",
    "# ====================\n",
    "dataset = KeplerDataset(\"train_ready.csv\", \"output_lightcurves\", seq_len=2000)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# ====================\n",
    "# Model\n",
    "# ====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using {device}\")\n",
    "\n",
    "tabular_dim = dataset.X.shape[1]\n",
    "model = DualBranchNN(tabular_dim).to(device)\n",
    "\n",
    "# use BCEWithLogitsLoss (more stable than BCELoss with sigmoid)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ====================\n",
    "# Training Loop\n",
    "# ====================\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    for flux, tab, labels in train_pbar:\n",
    "        flux = flux.to(device, non_blocking=True)\n",
    "        tab = tab.to(device, non_blocking=True)\n",
    "        labels = labels.float().unsqueeze(1).to(device, non_blocking=True)  # (B,1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(flux, tab).view(-1, 1)   # ensure (B,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for flux, tab, labels in val_pbar:\n",
    "            flux = flux.to(device, non_blocking=True)\n",
    "            tab = tab.to(device, non_blocking=True)\n",
    "            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(flux, tab).view(-1, 1)   # ensure (B,1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).int().cpu().numpy()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# ====================\n",
    "# Final Evaluation\n",
    "# ====================\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
